{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트맥스 회귀\n",
    "\n",
    "로지스틱 회귀는 이진분류,  즉 답이 맞냐 아니냐,  0또는 1을 판별하는 모델에 적합하다.\n",
    "\n",
    "그러면, 다중클래스 분류 문제는 어떻게 해결할 수 있을까?\n",
    "\n",
    "즉, 주어진 데이터로부터,  어느 '종류'인지를 도출해내는 다중분류문제는 소프트맥스를 통해서 실현할 수 있다.\n",
    "\n",
    "\n",
    "### one-hot encoding\n",
    "\n",
    "그러면, 다중분류문제에서는 어떤식으로 출력값을 할당하는게 좋을까?\n",
    "\n",
    "MSE cost를 생각해보면,\n",
    "\n",
    "$$\n",
    "MSE\\,\\,cost = \\frac{1}{n}\\sum_{i=1}^{n} (Y_i - y_i)^2\n",
    "$$\n",
    "\n",
    "여기서 $Y_i$는 실제값, $y_i$는 hypothesis라고 하자.\n",
    "\n",
    "문제는, 만약 $\\{y_1,y_2,y_3\\} = \\{1,2,3\\}$ 이런식으로 데이터 값을 설정하게되면,\n",
    "\n",
    "cost를 계산할때,  데이터간 Norm이 균등해지지 않는 문제가 발생한다. ( 왜냐면 MSE는 두 데이터의 제곱거리의 합으로 정의되므로)\n",
    "\n",
    "따라서, 이 문제를 해결하려면, 데이터간 거리가 균등하게 되도록 설정해야 한다.\n",
    "벡터공간에서 단위벡터와 비슷하게 설정하면 이러한 문제가 해결된다. 즉, \n",
    "\n",
    "$$\n",
    "y_1 = (1,0,0) \\,\\,\\, y_2 = (0,1,0) \\,\\,\\,  y_3 = (0,0,1)\n",
    "$$\n",
    "\n",
    "\n",
    "### Softmax function\n",
    "\n",
    "$$\n",
    "hypothesis = softmax( W^Tx +  b ) \n",
    "$$\n",
    "\n",
    "single layer NN : $W^Tx +  b$  에서 parameter $W \\,,\\,b$ 를 설정한 이후에,\n",
    "\n",
    "softmax를 통과하면, 결과값으로 $(0,0,\\dots,1,\\dots,0)$ 이 나오도록 하고 싶다. 즉,\n",
    "\n",
    "$$\n",
    "input = \\{x_1,x_2,x_3,x_4\\}\\\\\n",
    "\\newline\n",
    "softmax( W^T(input) + b ) =  \\{p_1,p_2,p_3\\}\\\\\n",
    "\\newline\n",
    "\\text{such that } p_1+p_2+p_3 = 1 \n",
    "$$\n",
    "\n",
    "\n",
    "시그모이드 함수처럼, 다음과 같은 함수를 차용해보자.\n",
    "$$\n",
    "Softmax(\\{z_i\\}_{i=1\\dots n}) = \\{\\frac{e^{z_i}}{\\sum_{i=1}^{n}e^{z_i}}\\}\n",
    "$$\n",
    "\n",
    "이 함수는 어떤 출력벡터 $\\{5,1,2\\}$ 를 받으면, 이를 합이 1이 나오는 이산확률변수로 만들어준다. 즉,\n",
    "$$\n",
    "e^5 + e^1 + e^2 \\approx 150.736\\\\\n",
    "Softmax(5,1,2) = (e^5/150.7 , e^1/150.7 , e^2/150.7) \\\\\n",
    "\\approx (0.94, 0.02, 0.05)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "![Alt text](../images/softmax_fig.PNG)\n",
    "\n",
    "\n",
    "### 왜 exponential 인가?\n",
    "\n",
    "합이 1이되는 수많은 형태중에서, exponential을 차용한 이유는\n",
    "\n",
    "softmax Activation function을 지나기 전,  $z = {z_1,z_2,...}$ 값이\n",
    "조금만 차이나더라도, 확률적으로는 매우 큰 차이를 보인다. 예를들어서, 앞서 계산한 케이스에서\n",
    "\n",
    "\n",
    "$$\n",
    "p_1 : p_2 : p_3 = e^5 : e^1 : e^2 = 94 : 2 : 5\n",
    "$$\n",
    "\n",
    "\n",
    "따라서, 적은 z 값의 변화에도,  확률분포는 큰 변화를 보인다.\n",
    "\n",
    "우리의 목적은, cost function을 최소화하는 parameter를 찾는 것이다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function of Softmax regression ;  Cross entropy\n",
    "\n",
    "\\begin{align*}\n",
    "    cost(W) &= -\\sum_{j=1}^{k}y_j\\,log(p_j)\\\\\n",
    "    &= -y_1\\,log(p_1) -y_2\\,log(p_2) - \\dots - y_k\\,log(p_k)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\\text{such that } p_1+p_2+\\dots+p_k = 1\n",
    "$$\n",
    "\n",
    "\n",
    "여기에서 cost(W) 는 하나의 one-hot vector에 대한 cost이다. 즉,\n",
    "만약 정답이 (0,1,0) 이라고 해보자. 그러면, cost function은\n",
    "$$\n",
    "cost(W) = -y_1\\,log(p_1) -y_2\\,log(p_2) - y_3\\,log(p_3)\n",
    "$$ \n",
    "이므로, $-log(p_2)$ 값에 따라 결정된다. 즉, $p_2$가 1에 가까워질수록 cost가 작아지고, $p_2$가 0에 가까워질 수록 cost가 커지게 된다.\n",
    "\n",
    "나머지 one-hot vector에 대해서도 마찬가지가 된다.\n",
    "\n",
    "\n",
    "### n개의 데이터에 대한 cost function\n",
    "\n",
    "우리는 현재 n개의 분류데이터가 존재한다. 즉, n개의 input vector\n",
    "$$ \n",
    "(x_{i1},x_{i2},x_{i3})_{i=1\\dots n} = \\{{\\footnotesize 꽃잎넓이,수명,색깔}\\}\\\\[0.3cm]\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "x_{11} & x_{12} & x_{13}\\\\\n",
    "x_{21} & x_{22} & x_{23}\\\\\n",
    "x_{31} & x_{32} & x_{33}\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "x_{n1} & x_{n2} & x_{n3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "따라서 n개의 꽃 데이터를 통해서, 각 꽃이 어느 종류인지에 대한 분류문제 ( softmax regression) 을 하고 싶은 것이다.\n",
    "\n",
    "그래서 이 데이터 행렬 $X$ 에다가, single layer NN을 취해주면, \n",
    "$XW + b$ 가 되고,  이 결과에다 softmax 함수를 취해주면, n개의 꽃에 대한 확률분포표가 만들어진다.\n",
    "\n",
    "$$ \n",
    "Softmax(XW + b ) \\\\[0.3cm]\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "p_{11} & p_{12} & p_{13}\\\\\n",
    "p_{21} & p_{22} & p_{23}\\\\\n",
    "p_{31} & p_{32} & p_{33}\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "p_{n1} & p_{n2} & p_{n3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "softmax 함수를 취해주면 $3 \\times n$ 행렬에서, 각 행 = 하나의 꽃\n",
    "을 exponential처리하는 결과가 된다. (즉, $p_{i1}+p_{i2}+p_{i3}=1$)\n",
    "\n",
    "\n",
    "우리는, 이 n개의 꽃 데이터 전체에 대한 cost가 가장 작아지는 W,b를 찾아야한다.\n",
    "\n",
    "따라서, 우리는 각 행마다 전부 cost fucntion을 구할 필요가 있다. 즉,\n",
    "$$\n",
    "cost(W) = \\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{3}-y_j\\,log(p_j)\n",
    "$$\n",
    "\n",
    "이 의미는, n개의 꽃데이터 각각에 대해서, cost를 구하겠다는 말이고,\n",
    "\n",
    "n=2번 에서, 답이 (0,1,0) 이었으면, $p_2$ 가 1에 가깝게 $W,b$를 조정해야 한다는 뜻이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Softmax regression using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x757480d08630>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 회귀에 필요한 각종 함수들이 들어있음.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 3 x 5 행렬을 만들어서 5개의 특성을 가지는 3개의 샘플 데이터를 만든다.\n",
    "z = torch.rand(3, 5, requires_grad=True) # requires_grad=True : 역전파 과정동안 이 텐서에 대한 기울기를 저장하겠다는 의미\n",
    "\n",
    "# hypothesis 계산\n",
    "hypothesis = F.softmax(z, dim=1) # dim=0 : column-wise, dim=1 : row-wise\n",
    "\n",
    "print(hypothesis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 크기가 5인 임의의 1차원 텐서 만들기\n",
    "\n",
    "y = torch.randint(5, (3,)).long() # 0 ~ 4 사이의 숫자 3개를 랜덤으로 뽑아 1차원 텐서로 만듦\n",
    "\n",
    "'''\n",
    "torch.randint(low, high, size) : low ~ high 사이의 숫자를 size 크기로 랜덤하게 뽑아 텐서로 만듦\n",
    "\n",
    "torch.randint(5, (3,)) : 최소값 : 디폴트로 0, 최대: 4 , 텐서타입: 1차원 텐서, 크기: 3\n",
    "'''\n",
    "\n",
    "# 모든 원소가 0의 값을 가진 3 × 5 텐서 생성\n",
    "y_one_hot = torch.zeros_like(hypothesis)  # 입력텐서랑 크기가 같은 0으로 채워진 텐서 생성\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1) \n",
    "\n",
    "# y.scatter_(dim, index, src) : y텐서에서 src의 값을 index에 따라 dim에 채워넣는다.\n",
    "# _가 붙으면 inplace 연산을 의미함 즉, y_one_hot에 바로 적용됨\n",
    "# y.unsqueeze(1) : y 텐서를 1차원에서 2차원으로 변경 (3,) -> (3, 1) ( 0은 첫번째 차원, 1은 두번째 차원을 의미)\n",
    "\n",
    "# index는 (0,2,1)^T 이고, dim=1은 행을 의미한다. src=1는 넣을 값을 의미한다.\n",
    "# 이때, 행에 어느위치에 1을 넣을 지는 index=(0,2,1)^T 에 의해 결정된다.\n",
    "# 즉, 0행에 0, 1행에 2, 2행에 1을 넣는다.\n",
    "\n",
    "print(y_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function을 구해보자.\n",
    "\n",
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1)\n",
    "# dim=1 은 행을 의미한다. 즉, 행을 기준으로 sum을 수행한다.\n",
    "\n",
    "cost = cost.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{pmatrix}\n",
    "1&0&0&0&0\\\\\n",
    "0&0&1&0&0\\\\\n",
    "0&1&0&0&0\n",
    "\\end{pmatrix}\n",
    ".*\n",
    "-log\n",
    "\\begin{pmatrix}\n",
    "p_{11} & p_{12} & p_{13} & p_{14} & p_{15}\\\\\n",
    "p_{21} & p_{22} & p_{23} & p_{24} & p_{25}\\\\\n",
    "p_{31} & p_{32} & p_{33} & p_{34} & p_{35}\\\\\n",
    "\\end{pmatrix}\n",
    "\n",
    "\\\\[0.3cm]\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "-log(p_{11})&0&0&0&0\\\\\n",
    "0&0&-log(p_{23})&0&0\\\\\n",
    "0&-log(p_{32})&0&0&0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "cost = \\frac{1}{3}\\{sum(row1) + sum(row2) + sum(row3)\\}\\\\[0.1cm]\n",
    "= \\frac{1}{3}\\{-log(p_{11}) -log(p_{23}) -log(p_{32})\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러가지 함수로 cost 함수 구현하기\n",
    "\n",
    "F.log_softmax() , F.nll_loss() , F.cross_entropy() 등을 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(F.softmax(z, dim=1)) # log(hypothesis)와 같다. 혹은, log(p_i) 들을 구하는 것과 같음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위 식을 간단하게 하려면, log_softmax() 함수를 사용하면 된다.\n",
    "\n",
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.47 1.47 1.47\n"
     ]
    }
   ],
   "source": [
    "# 가장 low level에서 cost는 다음과 같이 구할 수 있다.( 위에서 구한 방법과 동일)\n",
    "cost1 = (y_one_hot * -torch.log(F.softmax(z,dim=1))).sum(dim=1).mean()\n",
    "\n",
    "# 위에서 도입한 log_softmax() 함수를 사용하면 다음과 같이 구할 수 있다.\n",
    "cost2 = (y_one_hot * -F.log_softmax(z, dim=1)).sum(dim=1).mean()\n",
    "\n",
    "# nll_loss() 함수를 사용해서 간단하게 표현하기\n",
    "cost3 = F.nll_loss(F.log_softmax(z, dim=1), y)  \n",
    "#y는 F.log_softmax()에서 각 행에 어느 위치가 1인지를 나타내는 인덱스이다.\n",
    "# y = {0, 2, 1} 이면, 0행에 0번째, 1행에 2번째, 2행에 1번째 위치를 선택하라는 의미이다.\n",
    "# 이러한 방식으로, 굳이 tensor를 조작해서 one-hot encoding을 만들 필요가 없다.\n",
    "\n",
    "print(round(cost1.item(),2), round(cost2.item(),2), round(cost3.item(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax regression examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x757480d08630>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련데이터\n",
    "\n",
    "# 4개의 특성을 가지는 8개의 샘플\n",
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "\n",
    "# 각 샘플이 0, 1, 2 중 어떤 클래스에 속하는지를 나타내는 레이블\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "\n",
    "# 텐서로 변환\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding ( low level coding 일때만 사용)\n",
    "\n",
    "y_one_hot = torch.zeros(8, 3) #8개의 샘플당 3개의 클래스이므로\n",
    "\n",
    "# 각 행에서 정답에 해당하는 위치에 1을 넣는다.\n",
    "# (2,2,2,1,1,1,0,0)\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.704199\n",
      "Epoch  200/1000 Cost: 0.622999\n",
      "Epoch  300/1000 Cost: 0.565717\n",
      "Epoch  400/1000 Cost: 0.515291\n",
      "Epoch  500/1000 Cost: 0.467661\n",
      "Epoch  600/1000 Cost: 0.421278\n",
      "Epoch  700/1000 Cost: 0.375402\n",
      "Epoch  800/1000 Cost: 0.329766\n",
      "Epoch  900/1000 Cost: 0.285072\n",
      "Epoch 1000/1000 Cost: 0.248155\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화 (parameter 초기화)\n",
    "\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros((1,3), requires_grad=True)\n",
    "\n",
    "# optimizer 설정 ( 어떤 방식으로 최소화할 것인지)\n",
    "optimizer = optim.SGD([W, b], lr=0.1) \n",
    "# SGD이용, W와 b를 최적화 대상으로 설정, learning rate = 0.1\n",
    "\n",
    "# 학습횟수 = 1000번 설정\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "\n",
    "\n",
    "    # 가설 ( b를 더할때, 자동으로 broadcating이 일어남)\n",
    "    # dim=1은 행을 기준으로 softmax를 적용한다는 의미\n",
    "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1)\n",
    "\n",
    "    # 비용 함수\n",
    "    cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.704199\n",
      "Epoch  200/1000 Cost: 0.623000\n",
      "Epoch  300/1000 Cost: 0.565717\n",
      "Epoch  400/1000 Cost: 0.515291\n",
      "Epoch  500/1000 Cost: 0.467662\n",
      "Epoch  600/1000 Cost: 0.421278\n",
      "Epoch  700/1000 Cost: 0.375402\n",
      "Epoch  800/1000 Cost: 0.329766\n",
      "Epoch  900/1000 Cost: 0.285073\n",
      "Epoch 1000/1000 Cost: 0.248155\n"
     ]
    }
   ],
   "source": [
    "# high level로 구현하기; using F.cross_entropy()\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros((1,3), requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # single layer NN 구현, softmax는 아래에서 계산\n",
    "    z = x_train.matmul(W) + b\n",
    "\n",
    "    # 비용 함수( log_softmax() + nll_loss() = cross_entropy() )\n",
    "    # y_train위치에 해당하도록 하는것이 one-hot encoding을 대체.\n",
    "    # 즉, y_train을 인덱스로해서, 해당 위치 z값들에 log_softmax를 적용함.\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Modoule 과 class 사용해서 softmax 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [4., 3., 2., 1.]])\n",
      "\n",
      "Weights:\n",
      "Parameter containing:\n",
      "tensor([[-0.0450,  0.0725, -0.0020,  0.4371],\n",
      "        [ 0.1556, -0.1862, -0.3020, -0.0838],\n",
      "        [-0.2157, -0.1602,  0.0239,  0.2981]], requires_grad=True)\n",
      "\n",
      "Bias:\n",
      "Parameter containing:\n",
      "tensor([ 0.2718, -0.4888,  0.3100], requires_grad=True)\n",
      "\n",
      "Output:\n",
      "tensor([[ 2.1140, -1.9468,  1.0379],\n",
      "        [ 0.7422, -1.1128, -0.6874]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 모델을 선언 및 초기화.\n",
    "# XW + b 를 계산하는 모델이다.\n",
    "# 이 모델에서 W하고 b는 학습을 통해 변경될 수 있도록 랜덤으로 초기화된다.\n",
    "# 4 는 특성의 개수, 3은 클래스의 개수이다.\n",
    "model = nn.Linear(4, 3)\n",
    "\n",
    "# Example input tensor (batch_size=2, input_dim=4)\n",
    "x = torch.tensor([[1.0, 2.0, 3.0, 4.0], [4.0, 3.0, 2.0, 1.0]])\n",
    "\n",
    "# Perform a forward pass\n",
    "output = model(x)\n",
    "\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(\"\\nWeights:\")\n",
    "print(model.weight)\n",
    "print(\"\\nBias:\")\n",
    "print(model.bias)\n",
    "print(\"\\nOutput:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.366217\n",
      "Epoch  100/1000 Cost: 0.722726\n",
      "Epoch  200/1000 Cost: 0.637564\n",
      "Epoch  300/1000 Cost: 0.578576\n",
      "Epoch  400/1000 Cost: 0.527363\n",
      "Epoch  500/1000 Cost: 0.479316\n",
      "Epoch  600/1000 Cost: 0.432700\n",
      "Epoch  700/1000 Cost: 0.386693\n",
      "Epoch  800/1000 Cost: 0.340930\n",
      "Epoch  900/1000 Cost: 0.295757\n",
      "Epoch 1000/1000 Cost: 0.255350\n"
     ]
    }
   ],
   "source": [
    "# 내가 만들고 싶은 모델을 클래스를 통해서 다음과 같이 정의한다.\n",
    "# 이렇게 클래스를 통해서 모델을 설정하면, 여러가지 모델을 다룰때 편리하다.\n",
    "\n",
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # nn.Module에 있는 메소드를 상속받겠다.\n",
    "        self.linear = nn.Linear(4, 3) # Wx + b, 4개의 특성을 가지고 3개의 클래스로 분류한다.\n",
    "\n",
    "    # model이 학습 데이터를 받아서 forward 연산을 진행시키는 부분\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# softmax 분류기의 인스턴스를 만든다.    \n",
    "model = SoftmaxClassifierModel()\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산 (model에 x_train을 넣어서 예측값을 구함)\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml18)",
   "language": "python",
   "name": "ml18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
