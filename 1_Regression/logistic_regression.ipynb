{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가설식, W,b 정의하기\n",
    "\n",
    "시그모이드 함수는 \n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "이 함수를 사용하는 이유는,  이진분류에서  결국  나와야 하는 값은  0~1 사이의 확률이기 때문이다.  ( 이 함수의 모양은 0~1 사이를 매끄럽게 연결해주느 모양)\n",
    "\n",
    "결국 우리가 할 일은,  주어진 이진분류데이터 셋\n",
    ":  ( $x_i$, 0 또는 1)  \n",
    "들을 학습시켜서\n",
    "( 여기서 0~1은 강아지 사진일 확률,  데이터 $x_i$는 임의의 사진)\n",
    "\n",
    "내가 어떤 값을 입력했을때, 그 사진이 강아지일 확률을 뽑아내는 알고리즘을 만들고 싶은것.\n",
    "\n",
    "결과적으로, \n",
    "\n",
    "$$\n",
    "\\sigma( z = w^T X + b) = \\frac{1}{1+e^{-(w^T X + b)}} = \\text{강아지 사진일 확률}\n",
    "$$\n",
    "\n",
    "이 함수는 주어진 데이터, 파라미터에 대해서 0~1값을 주는 함수모델이된다.\n",
    "\n",
    "이제, W랑 b를 적절히 찾아내어서,  내가 입력한 데이터 셋 ( $x_i$ , 0 or 1) 과 최대한 경향성이 유사하게 만들면된다.\n",
    "\n",
    "### Cost function\n",
    "\n",
    "내가 위에서 만든 모델함수와,  주어진 이진 training data set  이 최대한 비슷하게 만들고 싶다.\n",
    "\n",
    "그러려면, cost function 을 정의해서  이거의 최소값을 구하면 된다.\n",
    "\n",
    "다만, 이 경우에는 MSE 를 쓰면 안된다. local minima가 너무 많아서 그렇다. 대신해서 log fucntion을 이용하여 cost funciton을 작성한다.\n",
    "\n",
    "$$\n",
    "cost = -[y\\,log\\sigma(z) + (1-y)\\,log(1-\\sigma(z))]\n",
    "$$\n",
    "\n",
    "이렇게 cost를 지정하는 이유는,\n",
    "\n",
    "1) $y=1$ 일때 (즉, 데이터 셋이 강아지 사진일때)  \n",
    "$\\sigma(z)$ 값이 1에 가까워 져야 == 강아지 사진일 확률이 커져야 cost가 작아진다.\n",
    "손실 함수가 최소가 되는 지점 = 강아지 사진일때 $\\sigma(z)$ 값이 1에 가까움.\n",
    "\n",
    "\n",
    "2. $y=0$ 일때 (즉, 데이터 셋이 강아지 사진이 아님)\n",
    "$\\sigma(z)$ 값이 0에 가까워 져야 == 강아지가 아닐 확률이 커져야 cost가 작아진다.\n",
    "손실 함수가 최소가 되는 지점 = 강아지 사진아닐때 $\\sigma(z)$ 값이 0에 가까움."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2x1 벡터는 가설식 W^T * X + b 에서 W^T에 해당한다.\n",
    "# 이진분류를 적절히 수행하기 위한 W와 b를 로지스틱 회귀를 통해서 찾아보자.\n",
    "\n",
    "\n",
    "W = torch.zeros((2, 1), requires_grad=True) \n",
    "# requires_grad=True : 학습을 통해 계속 값이 변경되는 변수임을 명시\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))\n",
    "# or hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "# 데이터셋 행렬 x_train과 W의 행렬곱에 b를 더한 값에 시그모이드 함수를 적용한 값이 hypothesis이다.\n",
    "\n",
    "print(hypothesis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서 정의한 비용함수는 F로 정의한 클래스에 이미 구현되어 있다.\n",
    "# F.binary_cross_entropy(예측값, 실제값)\n",
    "\n",
    "Cost = F.binary_cross_entropy(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.019834\n",
      "Epoch  100/1000 Cost: 0.018149\n",
      "Epoch  200/1000 Cost: 0.016730\n",
      "Epoch  300/1000 Cost: 0.015517\n",
      "Epoch  400/1000 Cost: 0.014469\n",
      "Epoch  500/1000 Cost: 0.013554\n",
      "Epoch  600/1000 Cost: 0.012748\n",
      "Epoch  700/1000 Cost: 0.012033\n",
      "Epoch  800/1000 Cost: 0.011394\n",
      "Epoch  900/1000 Cost: 0.010819\n",
      "Epoch 1000/1000 Cost: 0.010300\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정 ( 확률적 경사 하강법으로 최적화 방식 차용 )\n",
    "# lr 은 learning rate으로, 한번의 gradient decent당 얼마만큼 이동할지를 설정한다.\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "\n",
    "# 에포크는 Cost를 최소화하기 위해서 몇번 최적화를 반복할 것인지를 명시.\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train).mean()\n",
    "\n",
    "    # Reset gradients to zero\n",
    "    # 각 반복마다, 기울기를 0으로 초기화해야한다. 왜냐하면, 이전 반복에서 구한 기울기값이 남아있기 때문이다. \n",
    "    # ( 기본 설정은 기울기를 반복문마다 누적해서 합산하도록 되어있음 )\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute gradients of the cost with respect to parameters W and b\n",
    "    # cost함수는 W,b에 대한 함수이다. 따라서, cost를 W,b에 대해 미분하면 각각의 기울기를 구할 수 있다.\n",
    "    # backward() 함수를 통해 역전파 알고리즘을 통해서 cost함수의 현재지점에서의 기울기를 구할 수 있다.\n",
    "    cost.backward()\n",
    "\n",
    "    # Update parameters W and b using the computed gradients\n",
    "    # 이번 반복문에서 구한 기울기를 이용해서 W와 b를 업데이트한다.\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Explanation\n",
    "Start of the Loop:\n",
    "\n",
    "At the beginning of each iteration of the training loop, optimizer.zero_grad() is called to reset the gradients of all model parameters to zero.\n",
    "This ensures that the gradients from the previous iteration do not interfere with the current iteration.\n",
    "\n",
    "\n",
    "##### Forward Pass:\n",
    "\n",
    "The input data is passed through the model to compute the predictions (hypothesis).\n",
    "The loss (cost) is then calculated based on the predictions and the true labels.\n",
    "\n",
    "\n",
    "##### Backward Pass:\n",
    "\n",
    "The cost.backward() function is called to compute the gradients of the loss with respect to each model parameter.\n",
    "These gradients are stored in the .grad attribute of each parameter.\n",
    "\n",
    "\n",
    "##### Optimizer Step:\n",
    "\n",
    "The optimizer.step() function is called to update the model parameters using the computed gradients.\n",
    "The optimizer adjusts the parameters to minimize the loss.\n",
    "\n",
    "##### Why Gradients Are Not Reset During Backward Pass<br><br>\n",
    "Reset Before Backward Pass: The gradients are reset to zero before the backward pass using optimizer.zero_grad(). This ensures that the gradients computed during the backward pass are only based on the current iteration's loss.<br><br>\n",
    "Accumulate During Backward Pass: During the backward pass, the gradients are computed and accumulated in the .grad attribute of each parameter. Since the gradients were reset to zero at the beginning of the iteration, they only reflect the current iteration's computations.<br><br>\n",
    "Update After Backward Pass: After the backward pass, the optimizer updates the parameters using the accumulated gradients. The gradients are then reset again at the beginning of the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 완료후, 추가 데이터 입력후 값 예측단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7648e-04],\n",
      "        [3.1608e-02],\n",
      "        [3.8977e-02],\n",
      "        [9.5622e-01],\n",
      "        [9.9823e-01],\n",
      "        [9.9969e-01]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "print(hypothesis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True]])\n"
     ]
    }
   ],
   "source": [
    "# 예측값이 0.5보다 크면 True, 아니면 False로 설정\n",
    "# torch.FloatTensor 끼리의 크기비교는 각 원소별로 비교한 결과를 반환한다.\n",
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module / nn.Sequential 클래스 활용\n",
    "\n",
    "nn.Sequential은 인수로 nn.Module 객체 ( 여기서는 nn.Linear와 nn.Sigmoid )를 받아서 순서대로 연결하여 신경망을 생성한다.\n",
    "\n",
    "```python\n",
    "class Sequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__()\n",
    "        self.layers = nn.ModuleList(args)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "이렇게 nn.Sequential의 인수로 들어간 nn.Module의 객체는\n",
    "nn.Sequential내부 에서 nn.ModuleList에 저장된다.\n",
    "\n",
    "질문(1) 이렇게 층별로 쌓는 것은 어떤 이점이 존재하는가?\n",
    "\n",
    "만약에 내가 만드는 모델의 레이어가 많아지고 복잡해지면 다루는게 상당히 까다로워진다. 왜냐면, 각 레이어층마다 forward 연산 ( linear , Activation function 등) 을 지정해주어야 하기 때문. => 디버깅 및 보기에 좀 불편하다.\n",
    "\n",
    "\n",
    "##### nn.Sequential을 사용하지 않고 직접 만드는 경우\n",
    "```python\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=30, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=30*5*5, out_features=128, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10, bias=True)\n",
    "\n",
    "\n",
    "    # 이렇게 모든 레이어들에 대해서 한번에 forward 연산들을 지정해주어야 한다.\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 레이어 1\n",
    "        x = F.relu(self.conv1(x), inplace=True)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "\n",
    "        # 레이어2\n",
    "        x = F.relu(self.conv2(x), inplace=True)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "\n",
    "        # 레이어3\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x), inplace=True)\n",
    "        x = F.relu(self.fc2(x), inplace=True)\n",
    "\n",
    "        return x\n",
    "```\n",
    "\n",
    "\n",
    "##### nn.Sequential 을 사용해서 좀더 가독성 높게 표현하는 방법\n",
    "nn.Sequential is a convenient way to build neural networks by stacking layers sequentially. It allows you to define models in a clean, modular format.\n",
    "\n",
    "```python\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "\n",
    "\n",
    "        # 각 레이어들을 nn.Sequential을 이용해서 순차적으로 쌓는다. ( 여기에서는, 레이어1은 conv -> ReLU -> Maxpool 순으로 쌓아진다.)\n",
    "        # 이렇게 하면, 깊은 DNN 을 만들때 편하게 만들 수 있다.\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=30, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(in_features=30*5*5, out_features=128, bias=True),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(in_features=128, out_features=10, bias=True),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    # 각 레이어를 만들때, Sequential을 통해서 만들었다.\n",
    "    # forward 연산을 통해서 NN의 구조를 형성할때, 보다 간편하고 명시적으로 쌓을 수 있도록 한다.\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        return x\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Sequential을 통해서, 레이어를 순차적으로 쌓아서 NN 모델을 쉽게 구현한다.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 이 모델은, 입력노드2개 출력노드1개 인 NN에서 마지막 Activation Function으로 Sigmoid를 사용한 모델이다.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      5\u001b[0m    nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;66;03m# input_dim = 2, output_dim = 1\u001b[39;00m\n\u001b[1;32m      6\u001b[0m    nn\u001b[38;5;241m.\u001b[39mSigmoid() \u001b[38;5;66;03m# 출력은 시그모이드 함수를 거친다\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m     11\u001b[0m model(x_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Sequential을 통해서, 레이어를 순차적으로 쌓아서 NN 모델을 쉽게 구현한다.\n",
    "# 이 모델은, 입력노드2개 출력노드1개 인 NN에서 마지막 Activation Function으로 Sigmoid를 사용한 모델이다.\n",
    "\n",
    "model = nn.Sequential(\n",
    "   nn.Linear(2, 1), # input_dim = 2, output_dim = 1\n",
    "   nn.Sigmoid() # 출력은 시그모이드 함수를 거친다\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서 model이라는 nn.Sequential 클래스의 인스턴스를 작성한다.\n",
    "\n",
    "이 model의 인스턴스에 x_train데이터를 전달하면,  이 데이터는 \n",
    "nn.Sequential에 정의된 forward method를 통해서 자동으로 아까 정의한 레이어( nn.Linear ,nn.Sigmoid )들로 보내진다.\n",
    "\n",
    "( 이 forward 메소드는 model이 선언될때, 자동으로 실행된다. \n",
    "상속받은 nn.Module 클래스에는 __call__ 메소드가 있는데 얘가 인스턴스가 받는 인수를 자동으로 forward로 전달해준다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.539713 Accuracy 83.33%\n",
      "Epoch   10/1000 Cost: 0.614853 Accuracy 66.67%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   20/1000 Cost: 0.441875 Accuracy 66.67%\n",
      "Epoch   30/1000 Cost: 0.373145 Accuracy 83.33%\n",
      "Epoch   40/1000 Cost: 0.316358 Accuracy 83.33%\n",
      "Epoch   50/1000 Cost: 0.266094 Accuracy 83.33%\n",
      "Epoch   60/1000 Cost: 0.220498 Accuracy 100.00%\n",
      "Epoch   70/1000 Cost: 0.182095 Accuracy 100.00%\n",
      "Epoch   80/1000 Cost: 0.157299 Accuracy 100.00%\n",
      "Epoch   90/1000 Cost: 0.144091 Accuracy 100.00%\n",
      "Epoch  100/1000 Cost: 0.134272 Accuracy 100.00%\n",
      "Epoch  110/1000 Cost: 0.125769 Accuracy 100.00%\n",
      "Epoch  120/1000 Cost: 0.118297 Accuracy 100.00%\n",
      "Epoch  130/1000 Cost: 0.111680 Accuracy 100.00%\n",
      "Epoch  140/1000 Cost: 0.105779 Accuracy 100.00%\n",
      "Epoch  150/1000 Cost: 0.100483 Accuracy 100.00%\n",
      "Epoch  160/1000 Cost: 0.095704 Accuracy 100.00%\n",
      "Epoch  170/1000 Cost: 0.091369 Accuracy 100.00%\n",
      "Epoch  180/1000 Cost: 0.087420 Accuracy 100.00%\n",
      "Epoch  190/1000 Cost: 0.083806 Accuracy 100.00%\n",
      "Epoch  200/1000 Cost: 0.080486 Accuracy 100.00%\n",
      "Epoch  210/1000 Cost: 0.077425 Accuracy 100.00%\n",
      "Epoch  220/1000 Cost: 0.074595 Accuracy 100.00%\n",
      "Epoch  230/1000 Cost: 0.071969 Accuracy 100.00%\n",
      "Epoch  240/1000 Cost: 0.069526 Accuracy 100.00%\n",
      "Epoch  250/1000 Cost: 0.067248 Accuracy 100.00%\n",
      "Epoch  260/1000 Cost: 0.065118 Accuracy 100.00%\n",
      "Epoch  270/1000 Cost: 0.063122 Accuracy 100.00%\n",
      "Epoch  280/1000 Cost: 0.061247 Accuracy 100.00%\n",
      "Epoch  290/1000 Cost: 0.059483 Accuracy 100.00%\n",
      "Epoch  300/1000 Cost: 0.057820 Accuracy 100.00%\n",
      "Epoch  310/1000 Cost: 0.056250 Accuracy 100.00%\n",
      "Epoch  320/1000 Cost: 0.054764 Accuracy 100.00%\n",
      "Epoch  330/1000 Cost: 0.053357 Accuracy 100.00%\n",
      "Epoch  340/1000 Cost: 0.052022 Accuracy 100.00%\n",
      "Epoch  350/1000 Cost: 0.050753 Accuracy 100.00%\n",
      "Epoch  360/1000 Cost: 0.049546 Accuracy 100.00%\n",
      "Epoch  370/1000 Cost: 0.048396 Accuracy 100.00%\n",
      "Epoch  380/1000 Cost: 0.047299 Accuracy 100.00%\n",
      "Epoch  390/1000 Cost: 0.046252 Accuracy 100.00%\n",
      "Epoch  400/1000 Cost: 0.045251 Accuracy 100.00%\n",
      "Epoch  410/1000 Cost: 0.044294 Accuracy 100.00%\n",
      "Epoch  420/1000 Cost: 0.043376 Accuracy 100.00%\n",
      "Epoch  430/1000 Cost: 0.042497 Accuracy 100.00%\n",
      "Epoch  440/1000 Cost: 0.041653 Accuracy 100.00%\n",
      "Epoch  450/1000 Cost: 0.040843 Accuracy 100.00%\n",
      "Epoch  460/1000 Cost: 0.040064 Accuracy 100.00%\n",
      "Epoch  470/1000 Cost: 0.039315 Accuracy 100.00%\n",
      "Epoch  480/1000 Cost: 0.038593 Accuracy 100.00%\n",
      "Epoch  490/1000 Cost: 0.037898 Accuracy 100.00%\n",
      "Epoch  500/1000 Cost: 0.037228 Accuracy 100.00%\n",
      "Epoch  510/1000 Cost: 0.036582 Accuracy 100.00%\n",
      "Epoch  520/1000 Cost: 0.035958 Accuracy 100.00%\n",
      "Epoch  530/1000 Cost: 0.035356 Accuracy 100.00%\n",
      "Epoch  540/1000 Cost: 0.034773 Accuracy 100.00%\n",
      "Epoch  550/1000 Cost: 0.034210 Accuracy 100.00%\n",
      "Epoch  560/1000 Cost: 0.033664 Accuracy 100.00%\n",
      "Epoch  570/1000 Cost: 0.033137 Accuracy 100.00%\n",
      "Epoch  580/1000 Cost: 0.032625 Accuracy 100.00%\n",
      "Epoch  590/1000 Cost: 0.032130 Accuracy 100.00%\n",
      "Epoch  600/1000 Cost: 0.031649 Accuracy 100.00%\n",
      "Epoch  610/1000 Cost: 0.031183 Accuracy 100.00%\n",
      "Epoch  620/1000 Cost: 0.030730 Accuracy 100.00%\n",
      "Epoch  630/1000 Cost: 0.030291 Accuracy 100.00%\n",
      "Epoch  640/1000 Cost: 0.029864 Accuracy 100.00%\n",
      "Epoch  650/1000 Cost: 0.029449 Accuracy 100.00%\n",
      "Epoch  660/1000 Cost: 0.029046 Accuracy 100.00%\n",
      "Epoch  670/1000 Cost: 0.028654 Accuracy 100.00%\n",
      "Epoch  680/1000 Cost: 0.028272 Accuracy 100.00%\n",
      "Epoch  690/1000 Cost: 0.027900 Accuracy 100.00%\n",
      "Epoch  700/1000 Cost: 0.027538 Accuracy 100.00%\n",
      "Epoch  710/1000 Cost: 0.027186 Accuracy 100.00%\n",
      "Epoch  720/1000 Cost: 0.026842 Accuracy 100.00%\n",
      "Epoch  730/1000 Cost: 0.026507 Accuracy 100.00%\n",
      "Epoch  740/1000 Cost: 0.026181 Accuracy 100.00%\n",
      "Epoch  750/1000 Cost: 0.025862 Accuracy 100.00%\n",
      "Epoch  760/1000 Cost: 0.025552 Accuracy 100.00%\n",
      "Epoch  770/1000 Cost: 0.025248 Accuracy 100.00%\n",
      "Epoch  780/1000 Cost: 0.024952 Accuracy 100.00%\n",
      "Epoch  790/1000 Cost: 0.024663 Accuracy 100.00%\n",
      "Epoch  800/1000 Cost: 0.024381 Accuracy 100.00%\n",
      "Epoch  810/1000 Cost: 0.024104 Accuracy 100.00%\n",
      "Epoch  820/1000 Cost: 0.023835 Accuracy 100.00%\n",
      "Epoch  830/1000 Cost: 0.023571 Accuracy 100.00%\n",
      "Epoch  840/1000 Cost: 0.023313 Accuracy 100.00%\n",
      "Epoch  850/1000 Cost: 0.023061 Accuracy 100.00%\n",
      "Epoch  860/1000 Cost: 0.022814 Accuracy 100.00%\n",
      "Epoch  870/1000 Cost: 0.022572 Accuracy 100.00%\n",
      "Epoch  880/1000 Cost: 0.022336 Accuracy 100.00%\n",
      "Epoch  890/1000 Cost: 0.022104 Accuracy 100.00%\n",
      "Epoch  900/1000 Cost: 0.021877 Accuracy 100.00%\n",
      "Epoch  910/1000 Cost: 0.021655 Accuracy 100.00%\n",
      "Epoch  920/1000 Cost: 0.021437 Accuracy 100.00%\n",
      "Epoch  930/1000 Cost: 0.021224 Accuracy 100.00%\n",
      "Epoch  940/1000 Cost: 0.021015 Accuracy 100.00%\n",
      "Epoch  950/1000 Cost: 0.020810 Accuracy 100.00%\n",
      "Epoch  960/1000 Cost: 0.020609 Accuracy 100.00%\n",
      "Epoch  970/1000 Cost: 0.020412 Accuracy 100.00%\n",
      "Epoch  980/1000 Cost: 0.020219 Accuracy 100.00%\n",
      "Epoch  990/1000 Cost: 0.020029 Accuracy 100.00%\n",
      "Epoch 1000/1000 Cost: 0.019843 Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
    "\n",
    "        # 여기서 prediction은 torch.BoolTensor이다. 즉, tensor형태로 저장된 hypothesis를 >= 를 통해\n",
    "        # element-wise로 비교한 결과를 가지고 있다.\n",
    "\n",
    "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
    "\n",
    "        # 여기서, boolTensor를 floatTensor로 변환해야, y_train과 비교가 가능하다.\n",
    "\n",
    "\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "\n",
    "        # 먼저 correct_prediction에는 0,1이 담긴 텐서이다. 여기서 sum()을 통해 1의 개수를 셀 수 있다.\n",
    "        # sum()을 통해서, 텐서 원소의 합이 담긴 1차원 텐서가 만들어진다. 이를 item()을 통해 파이썬 스칼라값을 꺼낸다.\n",
    "        # len(correct_prediction)은 전체 원소의 개수를 나타낸다. 이를 통해 정확도를 계산한다.\n",
    "\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 클래스를 이용해서, 나만의 모델 구현 형식을 만들어보기\n",
    "\n",
    "실제 머신러닝 혹은 신경망을 구성할때에는,  여러가지 알고리즘이나 회귀등을 내 입맛대로\n",
    "추가하거나 빼서 설계해야한다.\n",
    "\n",
    "이때, torch로부터 nn.Module에 정의된 여러가지 회귀나 함수들을 불러와야 하는데 이때 이렇게 하는 방식을 익혀두어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.614994 Accuracy 66.67%\n",
      "Epoch   10/1000 Cost: 0.747550 Accuracy 83.33%\n",
      "Epoch   20/1000 Cost: 0.633216 Accuracy 83.33%\n",
      "Epoch   30/1000 Cost: 0.538123 Accuracy 83.33%\n",
      "Epoch   40/1000 Cost: 0.450406 Accuracy 83.33%\n",
      "Epoch   50/1000 Cost: 0.366382 Accuracy 83.33%\n",
      "Epoch   60/1000 Cost: 0.287368 Accuracy 83.33%\n",
      "Epoch   70/1000 Cost: 0.219289 Accuracy 83.33%\n",
      "Epoch   80/1000 Cost: 0.173225 Accuracy 100.00%\n",
      "Epoch   90/1000 Cost: 0.151674 Accuracy 100.00%\n",
      "Epoch  100/1000 Cost: 0.140280 Accuracy 100.00%\n",
      "Epoch  110/1000 Cost: 0.131002 Accuracy 100.00%\n",
      "Epoch  120/1000 Cost: 0.122903 Accuracy 100.00%\n",
      "Epoch  130/1000 Cost: 0.115765 Accuracy 100.00%\n",
      "Epoch  140/1000 Cost: 0.109426 Accuracy 100.00%\n",
      "Epoch  150/1000 Cost: 0.103760 Accuracy 100.00%\n",
      "Epoch  160/1000 Cost: 0.098664 Accuracy 100.00%\n",
      "Epoch  170/1000 Cost: 0.094056 Accuracy 100.00%\n",
      "Epoch  180/1000 Cost: 0.089870 Accuracy 100.00%\n",
      "Epoch  190/1000 Cost: 0.086050 Accuracy 100.00%\n",
      "Epoch  200/1000 Cost: 0.082549 Accuracy 100.00%\n",
      "Epoch  210/1000 Cost: 0.079328 Accuracy 100.00%\n",
      "Epoch  220/1000 Cost: 0.076356 Accuracy 100.00%\n",
      "Epoch  230/1000 Cost: 0.073604 Accuracy 100.00%\n",
      "Epoch  240/1000 Cost: 0.071048 Accuracy 100.00%\n",
      "Epoch  250/1000 Cost: 0.068668 Accuracy 100.00%\n",
      "Epoch  260/1000 Cost: 0.066446 Accuracy 100.00%\n",
      "Epoch  270/1000 Cost: 0.064367 Accuracy 100.00%\n",
      "Epoch  280/1000 Cost: 0.062417 Accuracy 100.00%\n",
      "Epoch  290/1000 Cost: 0.060584 Accuracy 100.00%\n",
      "Epoch  300/1000 Cost: 0.058858 Accuracy 100.00%\n",
      "Epoch  310/1000 Cost: 0.057231 Accuracy 100.00%\n",
      "Epoch  320/1000 Cost: 0.055692 Accuracy 100.00%\n",
      "Epoch  330/1000 Cost: 0.054236 Accuracy 100.00%\n",
      "Epoch  340/1000 Cost: 0.052856 Accuracy 100.00%\n",
      "Epoch  350/1000 Cost: 0.051546 Accuracy 100.00%\n",
      "Epoch  360/1000 Cost: 0.050301 Accuracy 100.00%\n",
      "Epoch  370/1000 Cost: 0.049115 Accuracy 100.00%\n",
      "Epoch  380/1000 Cost: 0.047986 Accuracy 100.00%\n",
      "Epoch  390/1000 Cost: 0.046908 Accuracy 100.00%\n",
      "Epoch  400/1000 Cost: 0.045878 Accuracy 100.00%\n",
      "Epoch  410/1000 Cost: 0.044893 Accuracy 100.00%\n",
      "Epoch  420/1000 Cost: 0.043951 Accuracy 100.00%\n",
      "Epoch  430/1000 Cost: 0.043048 Accuracy 100.00%\n",
      "Epoch  440/1000 Cost: 0.042182 Accuracy 100.00%\n",
      "Epoch  450/1000 Cost: 0.041351 Accuracy 100.00%\n",
      "Epoch  460/1000 Cost: 0.040552 Accuracy 100.00%\n",
      "Epoch  470/1000 Cost: 0.039784 Accuracy 100.00%\n",
      "Epoch  480/1000 Cost: 0.039045 Accuracy 100.00%\n",
      "Epoch  490/1000 Cost: 0.038334 Accuracy 100.00%\n",
      "Epoch  500/1000 Cost: 0.037649 Accuracy 100.00%\n",
      "Epoch  510/1000 Cost: 0.036987 Accuracy 100.00%\n",
      "Epoch  520/1000 Cost: 0.036349 Accuracy 100.00%\n",
      "Epoch  530/1000 Cost: 0.035734 Accuracy 100.00%\n",
      "Epoch  540/1000 Cost: 0.035138 Accuracy 100.00%\n",
      "Epoch  550/1000 Cost: 0.034563 Accuracy 100.00%\n",
      "Epoch  560/1000 Cost: 0.034006 Accuracy 100.00%\n",
      "Epoch  570/1000 Cost: 0.033468 Accuracy 100.00%\n",
      "Epoch  580/1000 Cost: 0.032946 Accuracy 100.00%\n",
      "Epoch  590/1000 Cost: 0.032441 Accuracy 100.00%\n",
      "Epoch  600/1000 Cost: 0.031951 Accuracy 100.00%\n",
      "Epoch  610/1000 Cost: 0.031476 Accuracy 100.00%\n",
      "Epoch  620/1000 Cost: 0.031014 Accuracy 100.00%\n",
      "Epoch  630/1000 Cost: 0.030567 Accuracy 100.00%\n",
      "Epoch  640/1000 Cost: 0.030132 Accuracy 100.00%\n",
      "Epoch  650/1000 Cost: 0.029710 Accuracy 100.00%\n",
      "Epoch  660/1000 Cost: 0.029299 Accuracy 100.00%\n",
      "Epoch  670/1000 Cost: 0.028900 Accuracy 100.00%\n",
      "Epoch  680/1000 Cost: 0.028512 Accuracy 100.00%\n",
      "Epoch  690/1000 Cost: 0.028134 Accuracy 100.00%\n",
      "Epoch  700/1000 Cost: 0.027766 Accuracy 100.00%\n",
      "Epoch  710/1000 Cost: 0.027407 Accuracy 100.00%\n",
      "Epoch  720/1000 Cost: 0.027058 Accuracy 100.00%\n",
      "Epoch  730/1000 Cost: 0.026718 Accuracy 100.00%\n",
      "Epoch  740/1000 Cost: 0.026386 Accuracy 100.00%\n",
      "Epoch  750/1000 Cost: 0.026063 Accuracy 100.00%\n",
      "Epoch  760/1000 Cost: 0.025747 Accuracy 100.00%\n",
      "Epoch  770/1000 Cost: 0.025439 Accuracy 100.00%\n",
      "Epoch  780/1000 Cost: 0.025138 Accuracy 100.00%\n",
      "Epoch  790/1000 Cost: 0.024845 Accuracy 100.00%\n",
      "Epoch  800/1000 Cost: 0.024558 Accuracy 100.00%\n",
      "Epoch  810/1000 Cost: 0.024278 Accuracy 100.00%\n",
      "Epoch  820/1000 Cost: 0.024004 Accuracy 100.00%\n",
      "Epoch  830/1000 Cost: 0.023737 Accuracy 100.00%\n",
      "Epoch  840/1000 Cost: 0.023475 Accuracy 100.00%\n",
      "Epoch  850/1000 Cost: 0.023219 Accuracy 100.00%\n",
      "Epoch  860/1000 Cost: 0.022969 Accuracy 100.00%\n",
      "Epoch  870/1000 Cost: 0.022724 Accuracy 100.00%\n",
      "Epoch  880/1000 Cost: 0.022484 Accuracy 100.00%\n",
      "Epoch  890/1000 Cost: 0.022250 Accuracy 100.00%\n",
      "Epoch  900/1000 Cost: 0.022020 Accuracy 100.00%\n",
      "Epoch  910/1000 Cost: 0.021795 Accuracy 100.00%\n",
      "Epoch  920/1000 Cost: 0.021574 Accuracy 100.00%\n",
      "Epoch  930/1000 Cost: 0.021358 Accuracy 100.00%\n",
      "Epoch  940/1000 Cost: 0.021147 Accuracy 100.00%\n",
      "Epoch  950/1000 Cost: 0.020939 Accuracy 100.00%\n",
      "Epoch  960/1000 Cost: 0.020736 Accuracy 100.00%\n",
      "Epoch  970/1000 Cost: 0.020536 Accuracy 100.00%\n",
      "Epoch  980/1000 Cost: 0.020340 Accuracy 100.00%\n",
      "Epoch  990/1000 Cost: 0.020148 Accuracy 100.00%\n",
      "Epoch 1000/1000 Cost: 0.019960 Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        # super를 통해서, nn.Module 클래스의 속성(attribute)들을 상속받는다.\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    # forward 함수는 모델이 학습데이터를 입력받아서 forward 연산을 진행시키는 함수이다.\n",
    "\n",
    "    # foward 메소드는 인스턴스가 호출될때, 자동으로 실행된다. 따라서, 모델을 인스턴스화하고,\n",
    "    # 인스턴스에 입력값을 넣으면 자동으로 forward 연산이 진행된다.\n",
    "    # 이는 매우 중요한데, 그 이유는 모델이 학습데이터를 입력받아서 예측값을 내놓는 과정을 정의하기 때문이다.\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "    \n",
    "\n",
    "    # 여기서 볼 수  있듯, forward 연산은 우리의 모델인 H(x)=sigmoid(Wx+b)를 정의하고 있다.\n",
    "\n",
    "\n",
    "# model을 생성하고, model에 x_train을 넣어서 forward 연산을 진행한다.\n",
    "model = BinaryClassifier()\n",
    "\n",
    "\n",
    "# optimizer 설정 ; 확률적 경사하강법을 사용한다.\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산, x_train을 넣어서 forward 연산을 진행한다.\n",
    "    hypothesis = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
    "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml18)",
   "language": "python",
   "name": "ml18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
