{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../images/xor_gate_graph.PNG)\n",
    "\n",
    "[인공신경망 그림그리기 사이트](http://alexlenail.me/NN-SVG/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단층 퍼셉트론으로는 XOR 이진분류 문제를 해결할 수 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7273973822593689\n",
      "100 0.6931476593017578\n",
      "200 0.6931471824645996\n",
      "300 0.6931471824645996\n",
      "400 0.6931471824645996\n",
      "500 0.6931471824645996\n",
      "600 0.6931471824645996\n",
      "700 0.6931471824645996\n",
      "800 0.6931471824645996\n",
      "900 0.6931471824645996\n",
      "1000 0.6931471824645996\n",
      "1100 0.6931471824645996\n",
      "1200 0.6931471824645996\n",
      "1300 0.6931471824645996\n",
      "1400 0.6931471824645996\n",
      "1500 0.6931471824645996\n",
      "1600 0.6931471824645996\n",
      "1700 0.6931471824645996\n",
      "1800 0.6931471824645996\n",
      "1900 0.6931471824645996\n",
      "2000 0.6931471824645996\n",
      "2100 0.6931471824645996\n",
      "2200 0.6931471824645996\n",
      "2300 0.6931471824645996\n",
      "2400 0.6931471824645996\n",
      "2500 0.6931471824645996\n",
      "2600 0.6931471824645996\n",
      "2700 0.6931471824645996\n",
      "2800 0.6931471824645996\n",
      "2900 0.6931471824645996\n",
      "3000 0.6931471824645996\n",
      "3100 0.6931471824645996\n",
      "3200 0.6931471824645996\n",
      "3300 0.6931471824645996\n",
      "3400 0.6931471824645996\n",
      "3500 0.6931471824645996\n",
      "3600 0.6931471824645996\n",
      "3700 0.6931471824645996\n",
      "3800 0.6931471824645996\n",
      "3900 0.6931471824645996\n",
      "4000 0.6931471824645996\n",
      "4100 0.6931471824645996\n",
      "4200 0.6931471824645996\n",
      "4300 0.6931471824645996\n",
      "4400 0.6931471824645996\n",
      "4500 0.6931471824645996\n",
      "4600 0.6931471824645996\n",
      "4700 0.6931471824645996\n",
      "4800 0.6931471824645996\n",
      "4900 0.6931471824645996\n",
      "5000 0.6931471824645996\n",
      "5100 0.6931471824645996\n",
      "5200 0.6931471824645996\n",
      "5300 0.6931471824645996\n",
      "5400 0.6931471824645996\n",
      "5500 0.6931471824645996\n",
      "5600 0.6931471824645996\n",
      "5700 0.6931471824645996\n",
      "5800 0.6931471824645996\n",
      "5900 0.6931471824645996\n",
      "6000 0.6931471824645996\n",
      "6100 0.6931471824645996\n",
      "6200 0.6931471824645996\n",
      "6300 0.6931471824645996\n",
      "6400 0.6931471824645996\n",
      "6500 0.6931471824645996\n",
      "6600 0.6931471824645996\n",
      "6700 0.6931471824645996\n",
      "6800 0.6931471824645996\n",
      "6900 0.6931471824645996\n",
      "7000 0.6931471824645996\n",
      "7100 0.6931471824645996\n",
      "7200 0.6931471824645996\n",
      "7300 0.6931471824645996\n",
      "7400 0.6931471824645996\n",
      "7500 0.6931471824645996\n",
      "7600 0.6931471824645996\n",
      "7700 0.6931471824645996\n",
      "7800 0.6931471824645996\n",
      "7900 0.6931471824645996\n",
      "8000 0.6931471824645996\n",
      "8100 0.6931471824645996\n",
      "8200 0.6931471824645996\n",
      "8300 0.6931471824645996\n",
      "8400 0.6931471824645996\n",
      "8500 0.6931471824645996\n",
      "8600 0.6931471824645996\n",
      "8700 0.6931471824645996\n",
      "8800 0.6931471824645996\n",
      "8900 0.6931471824645996\n",
      "9000 0.6931471824645996\n",
      "9100 0.6931471824645996\n",
      "9200 0.6931471824645996\n",
      "9300 0.6931471824645996\n",
      "9400 0.6931471824645996\n",
      "9500 0.6931471824645996\n",
      "9600 0.6931471824645996\n",
      "9700 0.6931471824645996\n",
      "9800 0.6931471824645996\n",
      "9900 0.6931471824645996\n",
      "10000 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "# 단층 퍼셉트론, 2->1 :  XOR 문제 구현\n",
    "# XOR문제는, 아래 X,Y에 정의된 것처럼, 이진분류 문제인데,  [0,0] -> 0, [0,1] -> 1, [1,0] -> 1, [1,1] -> 0 으로 분류하는 문제이다.\n",
    "\n",
    "# 하지만, 위 사진에서 볼 수 있듯이, 이 데이터는 직선 1개로는 분류가 불가능하다. ( 단층 퍼셉트론은 직선 1개로 분류하는 모델이다. )\n",
    "# 결과적으로, cost는 0.7 정도로 줄어들지 않는다. \n",
    "\n",
    "torch.manual_seed(777)\n",
    "\n",
    "# 입,출력 데이터 행렬이다. 우리는 W,b행렬에 대해서 최적화 및 미분을 수행하므로, 여기에서는 requires_grad=True로 설정하지 않는다.\n",
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]])\n",
    "\n",
    "\n",
    "# nn.Linear에 자동으로 requires_grad=True가 설정된다.\n",
    "linear = nn.Linear(in_features=2, out_features= 1, bias=True)  # input_dim : dim of x = 2, output_dim : dim of y = 1 , Linear TF contains bias  \n",
    "\n",
    "# define sigmoid function\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "# 입력노드 2개 출력노드1개이고, activation function으로 sigmoid를 사용하는 모델을 정의\n",
    "model = nn.Sequential(linear, sigmoid)\n",
    "\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = nn.BCELoss()  # Binary Cross Entropy Loss : 이진분류문제이므로, Bernoulli Distribution을 따르는 Binary Cross Entropy Loss 사용\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)  # Stochastic Gradient Descent method 사용\n",
    "\n",
    "\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    # cost/loss function\n",
    "    cost_val = cost(hypothesis, Y)\n",
    "    cost_val.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost_val.item())\n",
    "\n",
    "# 결과적으로, cost는 0.7 정도로 줄어들지 않는다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다층퍼셉트론(MLP) 로 XOR 이진분류 문제 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다층 퍼셉트론으로 XOR 문제 해결하기; MLP (Multi Layer Perceptron)\n",
    "\n",
    "\n",
    "# 먼저 nn.Sequential()을 사용하여, 다층 퍼셉트론의 레이어를 쉽게 쌓을 수 있다.\n",
    "model = nn.Sequential(\n",
    "    # 1st layer\n",
    "    nn.Linear(2, 10, bias=True),  # input_layer = 2, hidden_layer1 = 10\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    # 2nd layer\n",
    "    nn.Linear(10, 10, bias=True),  # hidden_layer1 = 10, hidden_layer2 = 10\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    # 3rd layer\n",
    "    nn.Linear(10, 10, bias=True),  # hidden_layer2 = 10, hidden_layer3 = 10\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    # 4th layer\n",
    "    nn.Linear(10, 1, bias=True),  # hidden_layer3 = 10, output_layer = 1\n",
    "    nn.Sigmoid()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../images/MLP_xor.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6940630078315735\n",
      "100 0.6931129693984985\n",
      "200 0.6931090950965881\n",
      "300 0.6931048035621643\n",
      "400 0.6930999755859375\n",
      "500 0.6930946707725525\n",
      "600 0.6930886507034302\n",
      "700 0.6930818557739258\n",
      "800 0.6930739879608154\n",
      "900 0.6930649876594543\n",
      "1000 0.6930544376373291\n",
      "1100 0.6930420398712158\n",
      "1200 0.6930272579193115\n",
      "1300 0.6930093765258789\n",
      "1400 0.6929875612258911\n",
      "1500 0.6929603815078735\n",
      "1600 0.6929258704185486\n",
      "1700 0.6928809881210327\n",
      "1800 0.6928214430809021\n",
      "1900 0.6927394866943359\n",
      "2000 0.6926223039627075\n",
      "2100 0.6924464106559753\n",
      "2200 0.6921651363372803\n",
      "2300 0.6916764974594116\n",
      "2400 0.6907237768173218\n",
      "2500 0.6885190010070801\n",
      "2600 0.6817606091499329\n",
      "2700 0.6487736701965332\n",
      "2800 0.5389859080314636\n",
      "2900 0.3458298146724701\n",
      "3000 0.0177101269364357\n",
      "3100 0.0074298689141869545\n",
      "3200 0.004491740837693214\n",
      "3300 0.0031569942366331816\n",
      "3400 0.0024083692114800215\n",
      "3500 0.0019341467414051294\n",
      "3600 0.001608923077583313\n",
      "3700 0.0013730265200138092\n",
      "3800 0.0011946671875193715\n",
      "3900 0.0010554117616266012\n",
      "4000 0.0009439234854653478\n",
      "4100 0.0008527325699105859\n",
      "4200 0.0007768301293253899\n",
      "4300 0.0007128154975362122\n",
      "4400 0.0006580546032637358\n",
      "4500 0.0006107768276706338\n",
      "4600 0.000569552939850837\n",
      "4700 0.0005333207664079964\n",
      "4800 0.0005011950270272791\n",
      "4900 0.000472559331683442\n",
      "5000 0.00044688055641017854\n",
      "5100 0.0004237385292071849\n",
      "5200 0.0004028086259495467\n",
      "5300 0.00038369616959244013\n",
      "5400 0.0003662926610559225\n",
      "5500 0.00035031483275815845\n",
      "5600 0.00033560150768607855\n",
      "5700 0.00032204281887970865\n",
      "5800 0.0003094616113230586\n",
      "5900 0.00029780808836221695\n",
      "6000 0.0002869809977710247\n",
      "6100 0.0002768702106550336\n",
      "6200 0.00026742630871012807\n",
      "6300 0.0002585730399005115\n",
      "6400 0.0002502416609786451\n",
      "6500 0.0002424396516289562\n",
      "6600 0.00023505788703914732\n",
      "6700 0.00022813971736468375\n",
      "6800 0.0002215499698650092\n",
      "6900 0.00021533318795263767\n",
      "7000 0.0002094472001772374\n",
      "7100 0.00020385008247103542\n",
      "7200 0.00019856245489791036\n",
      "7300 0.00019351534137967974\n",
      "7400 0.0001886996324174106\n",
      "7500 0.00018407867173664272\n",
      "7600 0.00017970410408452153\n",
      "7700 0.00017551015480421484\n",
      "7800 0.0001715208636596799\n",
      "7900 0.0001677016553003341\n",
      "8000 0.00016406179929617792\n",
      "8100 0.00016053812578320503\n",
      "8200 0.00015715660993009806\n",
      "8300 0.00015394073852803558\n",
      "8400 0.00015081318269949406\n",
      "8500 0.00014780033961869776\n",
      "8600 0.00014492901391349733\n",
      "8700 0.0001421672641299665\n",
      "8800 0.0001394519640598446\n",
      "8900 0.0001369003439322114\n",
      "9000 0.00013439092435874045\n",
      "9100 0.00013198124361224473\n",
      "9200 0.0001296687696594745\n",
      "9300 0.00012742314720526338\n",
      "9400 0.00012524184421636164\n",
      "9500 0.00012315252388361841\n",
      "9600 0.00012109507224522531\n",
      "9700 0.00011909715249203146\n",
      "9800 0.00011721727787517011\n",
      "9900 0.00011533491488080472\n",
      "10000 0.00011356764298398048\n"
     ]
    }
   ],
   "source": [
    "# 다층 퍼셉트론에서 사용할 cost function과 optimizer를 정의\n",
    "cost_mlp = nn.BCELoss()\n",
    "optimizer_mlp = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "# 학습 진행\n",
    "for step in range(10001):\n",
    "\n",
    "    # gradient 초기화 : zero_grad()를 호출하지 않으면, 이전 에포크에서의 gradient가 누적되어 학습이 되지 않는다.\n",
    "    optimizer_mlp.zero_grad()\n",
    "\n",
    "    # 데이터 X를 신경망 모델의 forward 연산에 전달한다.\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    # cost/loss function\n",
    "    cost_val_mlp = cost_mlp(hypothesis, Y)\n",
    "    cost_val_mlp.backward()\n",
    "    optimizer_mlp.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost_val_mlp.item())\n",
    "\n",
    "# 결과적으로, cost는 0.0001 정도로 줄어들었다. 즉, 다층 퍼셉트론으로 XOR 문제를 해결할 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다층 퍼셉트론에서, 모델 출력값, 예측값, 실제값, 정확도 계산하기\n",
    "\n",
    "\n",
    "##### pytorch 의 자동미분(autograd)와 computational graph 이해하기\n",
    "\n",
    "파이토치에서 텐서에 requires_grad = True 설정을 통해서, 각 에포크 마다 gradient 를 추적한다.\n",
    "이 설정을 하게되면, torch.autograd 에서 autograd backward graph를 생성하게 되고, 이 연산 그래프를 통해서 미분을 수행한다. ( DAG: Directed Acyclic Graph)\n",
    "\n",
    "[pytorch Autograd and graph tutorial](https://youtu.be/MswxJw-8PvE?si=JyUjf56ws6ZDxGsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 출력값(Hypothesis):\n",
      "  [[9.5467069e-05]\n",
      " [9.9987733e-01]\n",
      " [9.9988163e-01]\n",
      " [1.1770295e-04]]\n",
      "모델의 예측값(Predicted):\n",
      "  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "실제값(Y):\n",
      "  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도(Accuracy):  1.0\n"
     ]
    }
   ],
   "source": [
    "# with torch.no_grad()를 하면, gradient 계산을 수행하지 않는다. 즉, 모델의 학습 파라미터가 업데이트되지 않는다.\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()  # elementwise comparison 이후, bool->float로 변환\n",
    "    \n",
    "    # Y랑 같은 값들을 비교해서 맞으면 1, 틀리면 0으로 변환 후 평균을 구하면 정확도가 된다.\n",
    "    # 정확도 = (맞은 개수) / (전체 개수) = 1/n * sum(1 or 0)\n",
    "    accuracy = (predicted == Y).float().mean() \n",
    "\n",
    "    # convert pytorch tensor to numpy array ( .detach().numpy() )\n",
    "    # .detach() : .numpy() 호출전에 그래프에서 분리시키는 역할을 한다. \n",
    "    # hypothesis는 requires_grad=True로 설정되어 있으므로, pytorch에서 computation graph에 포함되어 있다. \n",
    "    # 이를 파이썬 리스트로 변환하기 위해서( .numpy()) 먼저 .detach()를 호출하여, 그래프에서 분리시킨다.\n",
    "    print('모델의 출력값(Hypothesis):\\n ', hypothesis.detach().numpy())\n",
    "    print('모델의 예측값(Predicted):\\n ', predicted.detach().numpy())\n",
    "    print('실제값(Y):\\n ', Y.numpy())\n",
    "    print('정확도(Accuracy): ', accuracy.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml18)",
   "language": "python",
   "name": "ml18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
